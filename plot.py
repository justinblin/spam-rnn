import matplotlib.pyplot as plt
from collections import Counter
from dataset import MyDataset
import torch

# currently saved 750 4x model
train_losses = [0.022345484437600923, 0.026480530893343582, 0.01944205602332298, 0.023806724358391294, 0.005586442912658162, 0.009612390752098594, 0.014416106479483147, 0.013160334959084793, 0.02720217224369669, 0.014795571752856092, 0.02179763377174897, 0.02212597146970619, 0.02237122586047198, 0.01644812394261954, 0.02777380113634988, 0.01944089667526544, 0.011415038501525306, 0.021339162409637086, 0.02427121392027359, 0.030791511481584283, 0.01926000726356135, 0.0045501655077794965, 0.0063028407491949896, 0.007494712598819509, 0.013665074060476247, 0.012651395054018438, 0.013038220931302118, 0.01942223053360112, 0.019269891839493455, 0.018302171215988344, 0.021041674330878217, 0.01010964210626162, 0.008168102389747188, 0.01865463138782241, 0.0231796243044968, 0.009659112398318823, 0.022950205181345787, 0.013418343996358098, 0.010610035095823943, 0.005842676767897299]
test_losses = [0.03370918068002048, 0.02462364675827468, 0.023595734346578575, 0.011604223704968818, 0.0054730977387838055, 0.009631575919178983, 0.014300687060969482, 0.012373332339280835, 0.03193288485016474, 0.012898936465597654, 0.021365962045596286, 0.02203046446659616, 0.022176821717732515, 0.013390631417705188, 0.027587984511093334, 0.017936447440239832, 0.011313758146452914, 0.02112304935233351, 0.016895047894480205, 0.030374025203116804, 0.013344993629547181, 0.004468048977030343, 0.009248063268101559, 0.005330645974590309, 0.01345665977300697, 0.012500872652814262, 0.012882488217789678, 0.025172699081446374, 0.024985037614475705, 0.014093322317978383, 0.020730708569815265, 0.004586764700480838, 0.007814113325515713, 0.018338344321870873, 0.022593036469800834, 0.009052954356831129, 0.022746714385283445, 0.012202089921933569, 0.010514265225537228, 0.00584266869944012]      
learning_rates = [0.002, 0.001, 0.002, 0.002, 6.25e-05, 0.00025, 0.002, 0.00025, 0.002, 0.001, 6.25e-05, 6.25e-05, 0.002, 6.25e-05, 6.25e-05, 0.0005, 6.25e-05, 6.25e-05, 0.002, 0.002, 0.002, 6.25e-05, 0.002, 0.0005, 6.25e-05, 6.25e-05, 6.25e-05, 0.002, 0.002, 0.00025, 6.25e-05, 0.001, 0.001, 0.002, 0.000125, 0.0005, 6.25e-05, 0.0005, 6.25e-05, 6.25e-05]

# all_data, flawed
# tensor([[0.8082, 0.0582],
#         [0.0576, 0.0761]], device='cuda:0')
# 88.42152466367713% correct
# precision: 0.5691275000572205
# recall: 0.5664662718772888
# f1 score: 0.5677937865257263

train_metrics = ([0.5680759997895958, 0.48857870803766956, 0.44288196451908246, 0.34130458535914115, 0.3259092002074956, 0.3313016187361997, 0.32453797106532994, 0.3148919709489115, 0.31065967315633236, 0.30371956458855504, 0.258688417601723, 0.20366737352090136, 0.1863676303790675, 0.1792946593306453, 0.1914468222581291, 0.17348707012649395, 0.17970141062746833, 0.18698193701830781, 0.18034324774042867, 0.1741310986024992], [0.6788051128387451, 0.6953058242797852, 0.6964438557624817, 0.7388335466384888, 0.7513513565063477, 0.754196286201477, 0.7664295434951782, 0.7681365013122559, 0.7732574939727783, 0.7735419869422913, 0.7840682864189148, 0.7957325577735901, 0.7857753038406372, 0.7886202335357666, 0.7886201739311218, 0.7954481244087219, 0.7891891598701477, 0.7852062582969666, 0.7948790788650513, 0.7943100929260254], [0.6506550312042236, 0.6108202934265137, 0.6784741282463074, 0.7799999713897705, 0.7727272510528564, 0.7702265381813049, 0.7673758864402771, 0.776334822177887, 0.7918485999107361, 0.791304349899292, 0.7846952080726624, 0.7985074520111084, 0.7777777910232544, 0.7868852615356445, 0.7905491590499878, 0.8058748245239258, 0.7783191204071045, 0.7671894431114197, 0.7962732911109924, 0.7950310111045837], [0.12437395751476288, 0.29215359687805176, 0.20784643292427063, 0.3255425691604614, 0.3831385672092438, 0.3973289132118225, 0.451585978269577, 0.44908180832862854, 0.4540901780128479, 0.45575961470603943, 0.505008339881897, 0.5358931422233582, 0.5200334191322327, 0.5208681225776672, 0.5166944861412048, 0.5267112255096436, 0.5333890318870544, 0.530884861946106, 0.5350584387779236, 0.534223735332489], [0.2088297144640168, 0.39525693419656277, 0.31821089020242016, 0.4593639508524637, 0.5122767835752283, 0.5242291027416698, 0.5685759340159527, 0.569011121822336, 0.5771883467531721, 0.5783898434423137, 0.6145251363703578, 0.6413586285795848, 0.6233116817391023, 0.6268207046361499, 0.6249368937570217, 0.637052018399372, 0.6329866615634632, 0.6275284242294072, 0.6400399455933583, 0.6390414442316423])
test_metrics = ([0.401181379988424, 0.412842503417966, 0.4117399719886449, 0.32605259030154266, 0.3456370359762407, 0.31825632638745494, 0.3360525265013501, 0.2969398207564718, 0.3067681417277709, 0.2962556549868027, 0.20391047901835443, 0.19188861647811628, 0.2621984448482808, 0.20185424159744597, 0.1700827264940584, 0.17216431955668487, 0.17261380877501695, 0.17968885643464147, 0.1795464922941436, 0.16640428360148424], [0.8466367721557617, 0.865470826625824, 0.8695067167282104, 0.8632286787033081, 0.8704035878181458, 0.8632286787033081, 0.8686099052429199, 0.8623318076133728, 0.8695067167282104, 0.8739910125732422, 0.8753362894058228, 0.865470826625824, 0.8730942010879517, 0.8748879432678223, 0.873991072177887, 0.8686099052429199, 0.8708520531654358, 0.8690583109855652, 0.8672645688056946, 0.8690583109855652], [0.42953017354011536, float('nan'), 0.6451613306999207, 0.48878926038742065, 0.5297297239303589, 0.49042144417762756, 0.5150214433670044, 0.48780491948127747, 0.5162454843521118, 0.5355805158615112, 0.5426356792449951, 0.5, 0.540669858455658, 0.5425101518630981, 0.5366795659065247, 0.5124555230140686, 0.52173912525177, 0.514285683631897, 0.506944477558136, 0.5141844153404236], [0.4266666769981384, 0.0, 0.06666667014360428, 0.3633333444595337, 0.3266666829586029, 0.4266666769981384, 0.4000000059604645, 0.46666666865348816, 0.4766666889190674, 0.4766666889190674, 0.46666666865348816, 0.48333337903022766, 0.37666669487953186, 0.4466666877269745, 0.4633333683013916, 0.47999998927116394, 0.47999998927116394, 0.47999998927116394, 0.4866666793823242, 0.48333337903022766], [0.42809363687704116, float('nan'), 0.12084592787081186, 0.4168260194068203, 0.404123722119594, 0.4563279865573418, 0.4502814236059339, 0.47700172442423516, 0.495667255009742, 0.5044091797726656, 0.5017921245364125, 0.491525447358347, 0.44400787882059584, 0.48994518046210817, 0.4973166695427934, 0.49569707156406456, 0.4999999917991873, 0.49655170410906924, 0.49659866196371405, 0.4982818197603492])
learning_rates = [0.064, 0.064, 0.064, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.001, 0.001, 0.001, 0.001, 0.001]

# PRE TUNING

# tensor([[0.8040, 0.0695],
#         [0.0614, 0.0650]], device='cuda:0')
# 0.8690583109855652% correct
# precision: 0.5141844153404236
# recall: 0.48333337903022766
# f1 score: 0.4982818197603492
# TRAINING METRICS:
# tensor([[0.6122, 0.1587],
#         [0.0469, 0.1821]], device='cuda:0')
# 0.7943100929260254% correct
# precision: 0.7950310111045837
# recall: 0.534223735332489
# f1 score: 0.6390414442316423

# AFTER TUNING

# tensor([[0.7417, 0.0614],
#         [0.1238, 0.0731]], device='cuda:0')
# 0.8147982358932495% correct
# precision: 0.3712984025478363
# recall: 0.5433333516120911
# f1 score: 0.441136675141625

# Training f1 better than testing f1 (0.65 vs 0.5)
# Test precision (0.52) better than recall (0.48)
# Train precision (0.8) way better than train recall (0.53)

# 0.54 ham 0.46 spam w/ dropout and weight decay
# 0.505 ham 0.495 spam w/ just dropout
# 0 ham 1 spam w/ neither

plt.figure()
# plot all the training metrics
for i in range(len(train_metrics)):
    if i != 0 and i != 1:
        plt.plot(train_metrics[i])

# plot all the testing metrics
for i in range(len(test_metrics)):
    if i != 0 and i != 1 and i != 2 and i != 3:
        plt.plot(test_metrics[i])

# plot all the learning rates
# plt.plot(learning_rates)

plt.legend([# 'train loss', 
            # 'train accuracy', 
            'train precision', 'train recall',
            'train f1',
            # 'test loss', 
            # 'test accuracy', 
            # 'test precision', 'test recall', 
            'test f1',
            # 'learning rates'
            ])
plt.show()

# abnormal_batches = [[10427, 7685, 10288, 8303, 2707, 6793, 6883, 11040, 6914, 2031, 2462, 8047, 6209, 1774, 10049, 9738, 7025, 7250, 799, 10314, 579, 2967, 547, 2750, 2073, 3427, 6236, 1882, 3849, 4799, 9301, 9935, 3292, 4457, 9553, 7013, 3691, 6685, 8997, 3720, 8448, 3304, 3983, 4678, 8478, 10660, 8399, 3269, 38, 2961, 9059, 9071, 2744, 7665, 2421, 4536, 2912, 5641, 6852, 7230, 7597, 2810, 2451, 3283, 2313, 1859, 581, 3970, 3276, 4508, 6034, 1767, 6016, 1782, 435, 10805, 10647, 10204, 5331, 476, 9772, 10779, 5696, 9331, 9514, 275, 8021, 5182, 3424, 520, 10163, 9123, 10655, 10588, 6314, 5913, 5735, 1627, 8382, 2344, 637, 8306, 8394, 1468, 2345, 7204, 8456, 3088, 4847, 5385, 1122, 6353, 5816, 2977, 332, 893, 5349, 1091, 4289, 10308, 1242, 9986, 1889, 9132, 7415, 10073, 7267], [6683, 941, 1099, 2672, 7100, 10983, 4762, 10964, 9782, 10026, 6583, 7362, 7667, 6600, 8090, 7637, 799, 5835, 4747, 10501, 566, 7485, 9770, 6893, 72, 8671, 10150, 7102, 5232, 10440, 8798, 1306, 9759, 7374, 5124, 7514, 7401, 2598, 10582, 3423, 2748, 5196, 7927, 10839, 2117, 11027, 6425, 805, 7068, 4892, 9462, 2982, 850, 2881, 8240, 9255, 9775, 3712, 7605, 4667, 6476, 4428, 10445, 650, 2756, 9870, 7695, 1859, 2924, 5081, 4341, 9867, 9448, 2072, 4238, 6211, 7826, 3224, 8616, 1716, 14, 9500, 8267, 4604, 7890, 5464, 1380, 2376, 8159, 2482, 8883, 177, 763, 4584, 1769, 8557, 12, 8097, 5591, 3485, 2135, 10951, 3765, 982, 9326, 3983, 10155, 4395, 10924, 8323, 257, 1575, 10464, 1367, 4293, 5145, 10699, 10564, 1811, 6932, 4127, 4219, 1783, 4208, 1432, 8668, 4171, 7676, 9648, 1312, 7887, 11118, 4536, 9533, 1093, 8639, 11060, 580, 979, 8836, 4570, 2850, 4545, 1199, 100, 9993, 2226, 3703, 3426, 6638, 2869, 833, 7777, 6307, 3890, 10508, 5205, 1942, 6399, 5247, 9059, 4630, 9857, 5170, 8478, 7889, 3678, 5570, 2671, 5360, 1690, 4145, 4461, 6582, 2658, 7727, 116, 10911, 8299, 8208, 8350, 7745, 2818, 9566, 3681, 8597, 6062, 9318, 2269, 6091, 1337, 9733, 6894, 3435, 2292, 9000, 7383, 11005, 2310, 5683, 1319, 1071, 397, 2951, 9033, 3658, 6956, 5634, 2421, 8132, 2806, 5980, 4274, 4576, 9637, 706, 3858, 1615, 11116, 4908, 1209, 6342, 6339, 6626, 1595, 10942, 9933, 9602, 4836, 9354, 440, 4376, 9044, 6883, 6694, 10606, 6096, 10936, 6313, 7032, 8346, 6030, 5671, 166, 1895, 8953, 4512, 7150, 7201, 2628, 1409, 6200, 10180, 8768], [10025, 4577, 5460, 8211, 3502, 4838, 2143, 2292, 9424, 8493, 7474, 8686, 9516, 3864, 5593, 833, 675, 5911, 3925, 1165, 1120, 7369, 5723, 7849, 4440, 6963, 7627, 1156, 1523, 2016, 9289, 2097, 5453, 4435, 9136, 1409, 9463, 2881, 10219, 4542, 8119, 2391, 3765, 5040, 6206, 8262, 6437, 8051, 9118, 8798, 1859, 9662, 3807, 10492, 5909, 9438, 8353, 2246, 9488, 3723, 9472, 865, 8212, 8886, 9301, 7704, 5544, 570, 9588, 7955, 6531, 2897, 5298, 2811, 9334, 1987, 6370, 10082, 1821, 13, 307, 6640, 507, 4409, 10525, 2864, 8120, 805, 3681, 10810, 9647, 1407, 3191, 3393, 841, 1774, 7540, 720, 5145, 4003, 10075, 3100, 4462, 7170, 9745, 3513, 10453, 4737, 8360, 1897, 4001, 6228, 2899, 7481, 7076, 1174, 1309, 5548, 1913, 4367, 7464, 8134, 9821, 11060, 7038, 7194, 7904, 3575, 8439, 10920, 5119, 8278, 8555, 6093, 4563, 5528, 939, 10150, 2852, 7936, 10259, 4589, 5431, 1439, 10172, 4156, 6805, 9205, 8305, 10283, 1500, 9383, 3768, 1376, 8768, 5265, 1124, 9614, 6630, 2211, 10305, 6185, 2932, 8586, 3536, 5888, 7215, 7055, 4171, 3551, 6371, 1584, 4904, 9918, 3818, 37, 1399, 10885, 6757, 2250, 4558, 10988, 7150, 3094, 3435, 580, 10482, 4857, 2521, 9916, 673, 9864, 2754, 9163, 6183, 7201, 3143, 9494, 806, 10921, 10301, 7670, 2498, 1019, 4518, 10397, 11070, 852, 7765, 4529, 3745, 3951, 3760, 7726, 763, 5299, 3334, 2943, 7713, 1074, 5771, 3876, 9036, 6641, 9147, 5994, 1964, 6075, 7214, 6884, 6158, 1180, 3522, 8455, 7746, 6129, 7500, 3582, 9623, 9968, 3966, 9972, 4600, 3980, 6124, 3105, 4504, 4654, 7884, 4514, 2040, 5821, 959, 1655, 1835, 414, 5132, 8184, 1630, 1371, 4366, 5139, 1504, 5030, 4936, 9693, 2147, 3532, 527, 6797, 4259, 9618, 4736, 5549, 2076, 10652, 10544, 5570, 8281, 4449, 6781, 8159, 2622, 519, 6996, 6575, 1475, 9935, 5478, 463, 10262, 5979, 3419, 2956, 11102, 6399, 10421, 7666, 7889, 2785, 7695, 735, 449, 6718, 9613, 5832, 4502, 2421, 1075, 1282, 8017, 873, 4334, 6684, 10934, 2126, 4535, 10008], [3944, 4802, 10971, 7986, 3411, 5991, 4920, 9093, 3796, 1596, 3234, 6260, 5741, 5568, 5716, 141, 4585, 5430, 7069, 3, 520, 1211, 931, 137, 6784, 1352, 4049, 5736, 7451, 1975, 4678, 7429, 9012, 10377, 1483, 7622, 9962, 5012, 2678, 2578, 7216, 10328, 6456, 600, 4867, 6638, 4, 7640, 8749, 5796, 3791, 6610, 8208, 7835, 8295, 2781, 2947, 451, 284, 8188, 6168, 9131, 1008, 10373, 9637, 9402, 4466, 9535, 11046, 476, 7853, 9665, 2403, 1233, 3318, 3263, 527, 4238, 8439, 799, 7679, 5497, 3134, 1841, 4519, 3286, 8202, 7161, 9857, 5002, 4251, 10531, 9054, 9506, 10806, 8713, 10704, 2249, 4499, 9018, 4716, 3292, 4005, 7012, 9588, 7908, 1576, 10647, 5881, 1142, 803, 10670, 5, 1605, 5414, 1012, 2126, 4508, 2271, 9373, 275, 1904, 7629, 1260, 2840, 9020, 9334, 1660, 945, 977, 2421, 6519, 3623, 3875, 2091, 2315, 5249, 10814, 4907, 786, 9301, 4202, 4168, 6379, 7275, 4146, 6399, 1621, 9924, 1324, 4979, 9938, 9789, 1093, 4370, 7150, 4880, 1471, 652, 2851, 4378, 8087, 3300, 4299, 4235, 4434, 6598, 6721, 1050, 1622, 4110, 3369, 1814, 9364, 1434, 1432, 3782, 3426, 3268, 5372, 9860, 9785, 8699, 7006, 4200, 5370, 2744, 8072, 4589, 6793, 6538, 4559, 3980, 2839, 6187, 3677, 10092, 1320, 6117, 6179, 1859, 4689, 689, 2576, 176, 8969, 1046, 8975, 2624, 5259, 4916, 6728, 6945, 5793, 6047, 5811, 3011, 4212, 9618, 1513, 9147, 4001, 3506, 4531, 8643, 10093, 10237, 5964, 2076, 7785, 5944, 8522, 6803, 10659, 1419, 8163, 5289, 4006, 4226, 9116, 7282, 7907, 3157, 534, 1167, 593, 4969, 6173, 5895, 4597, 10636, 9669, 7596, 1196, 5513, 5381, 2150, 7373, 9023, 4373, 3294, 4588, 4703, 5825, 5396, 1174, 3128, 1317, 5629, 871, 5697, 8581, 8702, 283, 9768, 1625, 383, 9042, 3069, 2778, 768, 1231, 3392, 6530, 5771, 9461, 5214, 4726, 1890, 540, 6908, 4650, 10922, 10658, 1716, 2852, 742, 7140, 8030, 10368, 8656, 6837, 9335, 4331, 4375, 5498, 1179, 1246, 100, 733, 6990, 2707, 6404, 11064, 9489, 6327, 8534, 1581, 6943, 6627, 1629, 817, 6346, 4099, 4071, 3007, 3899, 4198, 2316, 7120, 4313, 10882, 637, 351, 9563, 5141, 3850, 2561, 7537, 7046, 2, 1107, 828, 10174, 8297, 11060, 10103, 6411, 253, 475, 4737, 6000, 9792, 2598, 3301, 2169, 3004, 1897, 4062, 10439, 1736, 790, 826, 1444, 7546, 5911, 4756, 1099, 10738, 7887, 10200, 5994, 6649, 5889, 2461, 840, 7554, 2057, 4044, 1665, 2527, 1282], [10090, 5232, 11060, 11109, 4023, 7251, 4184, 1575, 2421, 3908, 9130, 4900, 7962, 5596, 2634, 941, 5849, 8997, 8200, 10163, 2076, 4187, 5957, 268, 6834, 5539, 2693, 1777, 4558, 70, 3315, 2711, 10241, 8428, 2482, 4422, 8292, 9720, 5591, 5585, 7416, 7588, 7556, 1024, 4259, 1102, 8256, 10308, 595, 7658, 1219, 1546, 9782, 4221, 3676, 8568, 3426, 699, 10597, 11079, 5848, 4015, 8530, 9479, 1209, 6292, 3572, 9863, 7569, 141, 9649, 871, 9662, 10329, 2669, 5081, 2823, 50, 8980, 6437, 8632, 1085, 6050, 7737, 6433, 6608, 3132, 10957, 4870, 4720, 841, 2992, 10507, 6103, 5460, 7746, 2485, 5124, 2675, 6776, 154, 7429, 9044, 2828, 5236, 8410, 7324, 698, 9807, 4071, 489, 10715, 10485, 9962, 9619, 335, 11000, 1155, 468, 2097, 2850, 446, 8257, 10410, 7263, 10948, 7441, 4298, 4933, 5143, 4340, 10162, 3965, 2271, 8770, 5349, 5470, 7452, 9469, 7239, 8886, 5816, 5739, 440, 7947, 8457, 10234, 7361, 7877, 4429, 6094, 8373, 4724, 7189, 9667, 7372, 6837, 8488, 5505, 5073, 8581, 4964, 8512, 10259, 3015, 438, 7249, 9759, 5426, 8585, 8618, 4690, 2046, 3629, 3271, 10110, 4504, 4630, 937, 1859, 4338, 2318, 2148, 3660, 572, 9986, 1342, 7592, 4204, 1965, 7362]]
# counter = Counter()
# for cur_list in abnormal_batches:
#     counter.update(set(cur_list))
# fishy_examples = [elem for elem, count in counter.items() if count >= 3]
# print(fishy_examples)

# all_data = MyDataset([',', '\t'], ['data/kaggle spam.csv', 'data/UC Irvine collection/SMSSpamCollection'])
# for i in fishy_examples:
#     print(f'{all_data[i][2]}, {all_data[i][3]}')