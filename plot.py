import matplotlib.pyplot as plt
from collections import Counter
from dataset import MyDataset
import torch

# currently saved 750 4x model
train_losses = [0.022345484437600923, 0.026480530893343582, 0.01944205602332298, 0.023806724358391294, 0.005586442912658162, 0.009612390752098594, 0.014416106479483147, 0.013160334959084793, 0.02720217224369669, 0.014795571752856092, 0.02179763377174897, 0.02212597146970619, 0.02237122586047198, 0.01644812394261954, 0.02777380113634988, 0.01944089667526544, 0.011415038501525306, 0.021339162409637086, 0.02427121392027359, 0.030791511481584283, 0.01926000726356135, 0.0045501655077794965, 0.0063028407491949896, 0.007494712598819509, 0.013665074060476247, 0.012651395054018438, 0.013038220931302118, 0.01942223053360112, 0.019269891839493455, 0.018302171215988344, 0.021041674330878217, 0.01010964210626162, 0.008168102389747188, 0.01865463138782241, 0.0231796243044968, 0.009659112398318823, 0.022950205181345787, 0.013418343996358098, 0.010610035095823943, 0.005842676767897299]
test_losses = [0.03370918068002048, 0.02462364675827468, 0.023595734346578575, 0.011604223704968818, 0.0054730977387838055, 0.009631575919178983, 0.014300687060969482, 0.012373332339280835, 0.03193288485016474, 0.012898936465597654, 0.021365962045596286, 0.02203046446659616, 0.022176821717732515, 0.013390631417705188, 0.027587984511093334, 0.017936447440239832, 0.011313758146452914, 0.02112304935233351, 0.016895047894480205, 0.030374025203116804, 0.013344993629547181, 0.004468048977030343, 0.009248063268101559, 0.005330645974590309, 0.01345665977300697, 0.012500872652814262, 0.012882488217789678, 0.025172699081446374, 0.024985037614475705, 0.014093322317978383, 0.020730708569815265, 0.004586764700480838, 0.007814113325515713, 0.018338344321870873, 0.022593036469800834, 0.009052954356831129, 0.022746714385283445, 0.012202089921933569, 0.010514265225537228, 0.00584266869944012]      
learning_rates = [0.002, 0.001, 0.002, 0.002, 6.25e-05, 0.00025, 0.002, 0.00025, 0.002, 0.001, 6.25e-05, 6.25e-05, 0.002, 6.25e-05, 6.25e-05, 0.0005, 6.25e-05, 6.25e-05, 0.002, 0.002, 0.002, 6.25e-05, 0.002, 0.0005, 6.25e-05, 6.25e-05, 6.25e-05, 0.002, 0.002, 0.00025, 6.25e-05, 0.001, 0.001, 0.002, 0.000125, 0.0005, 6.25e-05, 0.0005, 6.25e-05, 6.25e-05]

# all_data, flawed
# tensor([[0.8082, 0.0582],
#         [0.0576, 0.0761]], device='cuda:0')
# 88.42152466367713% correct
# precision: 0.5691275000572205
# recall: 0.5664662718772888
# f1 score: 0.5677937865257263

train_metrics = ([0.5259907649735273, 0.5424454199210916, 0.5869479940720922, 0.4456683844289071, 0.42220818147043826, 0.4070332474705449, 0.4011934706098564, 0.3338439985157769, 0.3193471236521138, 0.36489747168187836, 0.31629294888681037, 0.36431495216632603, 0.30492814160698867, 0.30638428953666885, 0.30627853286779216, 0.3177389936116093, 0.308970323776173, 0.31732413725638486, 0.28463257287751587, 0.2630893176623087], [70.05276679992676, 70.29023766517639, 66.80738925933838, 71.02901935577393, 70.97625136375427, 72.8232204914093, 72.55936861038208, 74.53826069831848, 74.82849955558777, 72.34828472137451, 74.77572560310364, 72.98153042793274, 75.30342936515808, 74.72295761108398, 74.8021125793457, 75.03957748413086, 74.40633177757263, 75.14512538909912, 75.98944902420044, 75.461745262146], [0.6196318864822388, 0.5800000429153442, 0.45597484707832336, 0.5652920603752136, 0.5968586206436157, 0.6628959774971008, 0.6288461685180664, 0.6512300968170166, 0.663690447807312, 0.5746445655822754, 0.6552706956863403, 0.5935162305831909, 0.6695156693458557, 0.656521737575531, 0.6493860483169556, 0.6619318127632141, 0.6465517282485962, 0.6615599393844604, 0.6993957161903381, 0.672804594039917], [0.08603066205978394, 0.14821124076843262, 0.37052810192108154, 0.28023847937583923, 0.1942078322172165, 0.24957409501075745, 0.2785349190235138, 0.3833049237728119, 0.3798977732658386, 0.41311755776405334, 0.39182281494140625, 0.4054514169692993, 0.4003407061100006, 0.38586029410362244, 0.4054514169692993, 0.39693355560302734, 0.3833049237728119, 0.404599666595459, 0.3943781852722168, 0.404599666595459], [0.1510845135187032, 0.23609226589121696, 0.4088345838722669, 0.37471523488537056, 0.29305911895557724, 0.36262375864099494, 0.3860684755971402, 0.48257371215271105, 0.48320691661415693, 0.4806739475117521, 0.49040511840916745, 0.48178136153621076, 0.5010660906767462, 0.48605149177070545, 0.4992133899581437, 0.49627262512561865, 0.4812834105116918, 0.5021141851680809, 0.5043572770743255, 0.5053191719734053])
test_metrics = ([0.5327876433788409, 0.36097558892223947, 0.45388591844379667, 0.4174946207188876, 0.37889516266181855, 0.5356001477975112, 0.37835488616254526, 0.2852812683372485, 0.28819392902443897, 0.3494440202335335, 0.29776508065214985, 0.2939729401892758, 0.33863695779866315, 0.35563384222626837, 0.2859503036022497, 0.27630037927254547, 0.2837690319045115, 0.2742410373734371, 0.2794339038596781, 0.25334539807837514], [87.29147911071777, 82.80254602432251, 85.74461340904236, 86.1085832118988, 85.35032272338867, 64.60418701171875, 85.31998991966248, 85.13800501823425, 85.19867062568665, 86.1085832118988, 83.62147808074951, 82.46890902519226, 85.71428060531616, 79.19320464134216, 85.19866466522217, 85.16833782196045, 83.62147212028503, 85.28965711593628, 84.41007137298584, 84.71337556838989], [0.4117647111415863, 0.3273542523384094, 0.3836734890937805, 0.3636363446712494, 0.38333335518836975, 0.18842975795269012, 0.40220385789871216, 0.39895012974739075, 0.3999999761581421, 0.42320820689201355, 0.3546910881996155, 0.33128833770751953, 0.40645164251327515, 0.2780487835407257, 0.39669424295425415, 0.39839571714401245, 0.3579418361186981, 0.4027027189731598, 0.378313273191452, 0.38596490025520325], [0.033898308873176575, 0.35351091623306274, 0.22760291397571564, 0.1452784538269043, 0.2784503698348999, 0.5520581603050232, 0.35351091623306274, 0.36803874373435974, 0.36319613456726074, 0.300242155790329, 0.37530267238616943, 0.3922518193721771, 0.305084764957428, 0.4140436053276062, 0.34866830706596375, 0.36077481508255005, 0.3874092102050781, 0.36077481508255005, 0.38014528155326843, 0.37288135290145874], [0.06263982755800314, 0.33993015656753356, 0.2857142978020252, 0.20761245719325655, 0.32258065735448976, 0.28096119587431057, 0.3762886718248904, 0.3828715373623556, 0.38071065385684255, 0.35127481010389716, 0.3647058932834018, 0.359201771818852, 0.34854774125617394, 0.3326848341228016, 0.3711340477460858, 0.3786531090999109, 0.37209302841674124, 0.3805874894550089, 0.37922706482256646, 0.3793103374523087])
learning_rates = [0.064, 0.064, 0.064, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.004, 0.004]

# 0.54 ham 0.46 spam w/ dropout and weight decay
# 0.505 ham 0.495 spam w/ just dropout
# 0 ham 1 spam w/ neither

plt.figure()
# plot all the training metrics
for i in range(len(train_metrics)):
    if i != 0 and i != 1:
        plt.plot(train_metrics[i])

# plot all the testing metrics
for i in range(len(test_metrics)):
    if i != 0 and i != 1 and i != 2 and i != 3:
        plt.plot(test_metrics[i])

# plot all the learning rates
# plt.plot(learning_rates)

plt.legend([# 'train loss', 
            # 'train accuracy', 
            'train precision', 'train recall',
            'train f1',
            # 'test loss', 
            # 'test accuracy', 
            # 'test precision', 'test recall', 
            'test f1',
            # 'learning rates'
            ])
plt.show()

# abnormal_batches = [[10427, 7685, 10288, 8303, 2707, 6793, 6883, 11040, 6914, 2031, 2462, 8047, 6209, 1774, 10049, 9738, 7025, 7250, 799, 10314, 579, 2967, 547, 2750, 2073, 3427, 6236, 1882, 3849, 4799, 9301, 9935, 3292, 4457, 9553, 7013, 3691, 6685, 8997, 3720, 8448, 3304, 3983, 4678, 8478, 10660, 8399, 3269, 38, 2961, 9059, 9071, 2744, 7665, 2421, 4536, 2912, 5641, 6852, 7230, 7597, 2810, 2451, 3283, 2313, 1859, 581, 3970, 3276, 4508, 6034, 1767, 6016, 1782, 435, 10805, 10647, 10204, 5331, 476, 9772, 10779, 5696, 9331, 9514, 275, 8021, 5182, 3424, 520, 10163, 9123, 10655, 10588, 6314, 5913, 5735, 1627, 8382, 2344, 637, 8306, 8394, 1468, 2345, 7204, 8456, 3088, 4847, 5385, 1122, 6353, 5816, 2977, 332, 893, 5349, 1091, 4289, 10308, 1242, 9986, 1889, 9132, 7415, 10073, 7267], [6683, 941, 1099, 2672, 7100, 10983, 4762, 10964, 9782, 10026, 6583, 7362, 7667, 6600, 8090, 7637, 799, 5835, 4747, 10501, 566, 7485, 9770, 6893, 72, 8671, 10150, 7102, 5232, 10440, 8798, 1306, 9759, 7374, 5124, 7514, 7401, 2598, 10582, 3423, 2748, 5196, 7927, 10839, 2117, 11027, 6425, 805, 7068, 4892, 9462, 2982, 850, 2881, 8240, 9255, 9775, 3712, 7605, 4667, 6476, 4428, 10445, 650, 2756, 9870, 7695, 1859, 2924, 5081, 4341, 9867, 9448, 2072, 4238, 6211, 7826, 3224, 8616, 1716, 14, 9500, 8267, 4604, 7890, 5464, 1380, 2376, 8159, 2482, 8883, 177, 763, 4584, 1769, 8557, 12, 8097, 5591, 3485, 2135, 10951, 3765, 982, 9326, 3983, 10155, 4395, 10924, 8323, 257, 1575, 10464, 1367, 4293, 5145, 10699, 10564, 1811, 6932, 4127, 4219, 1783, 4208, 1432, 8668, 4171, 7676, 9648, 1312, 7887, 11118, 4536, 9533, 1093, 8639, 11060, 580, 979, 8836, 4570, 2850, 4545, 1199, 100, 9993, 2226, 3703, 3426, 6638, 2869, 833, 7777, 6307, 3890, 10508, 5205, 1942, 6399, 5247, 9059, 4630, 9857, 5170, 8478, 7889, 3678, 5570, 2671, 5360, 1690, 4145, 4461, 6582, 2658, 7727, 116, 10911, 8299, 8208, 8350, 7745, 2818, 9566, 3681, 8597, 6062, 9318, 2269, 6091, 1337, 9733, 6894, 3435, 2292, 9000, 7383, 11005, 2310, 5683, 1319, 1071, 397, 2951, 9033, 3658, 6956, 5634, 2421, 8132, 2806, 5980, 4274, 4576, 9637, 706, 3858, 1615, 11116, 4908, 1209, 6342, 6339, 6626, 1595, 10942, 9933, 9602, 4836, 9354, 440, 4376, 9044, 6883, 6694, 10606, 6096, 10936, 6313, 7032, 8346, 6030, 5671, 166, 1895, 8953, 4512, 7150, 7201, 2628, 1409, 6200, 10180, 8768], [10025, 4577, 5460, 8211, 3502, 4838, 2143, 2292, 9424, 8493, 7474, 8686, 9516, 3864, 5593, 833, 675, 5911, 3925, 1165, 1120, 7369, 5723, 7849, 4440, 6963, 7627, 1156, 1523, 2016, 9289, 2097, 5453, 4435, 9136, 1409, 9463, 2881, 10219, 4542, 8119, 2391, 3765, 5040, 6206, 8262, 6437, 8051, 9118, 8798, 1859, 9662, 3807, 10492, 5909, 9438, 8353, 2246, 9488, 3723, 9472, 865, 8212, 8886, 9301, 7704, 5544, 570, 9588, 7955, 6531, 2897, 5298, 2811, 9334, 1987, 6370, 10082, 1821, 13, 307, 6640, 507, 4409, 10525, 2864, 8120, 805, 3681, 10810, 9647, 1407, 3191, 3393, 841, 1774, 7540, 720, 5145, 4003, 10075, 3100, 4462, 7170, 9745, 3513, 10453, 4737, 8360, 1897, 4001, 6228, 2899, 7481, 7076, 1174, 1309, 5548, 1913, 4367, 7464, 8134, 9821, 11060, 7038, 7194, 7904, 3575, 8439, 10920, 5119, 8278, 8555, 6093, 4563, 5528, 939, 10150, 2852, 7936, 10259, 4589, 5431, 1439, 10172, 4156, 6805, 9205, 8305, 10283, 1500, 9383, 3768, 1376, 8768, 5265, 1124, 9614, 6630, 2211, 10305, 6185, 2932, 8586, 3536, 5888, 7215, 7055, 4171, 3551, 6371, 1584, 4904, 9918, 3818, 37, 1399, 10885, 6757, 2250, 4558, 10988, 7150, 3094, 3435, 580, 10482, 4857, 2521, 9916, 673, 9864, 2754, 9163, 6183, 7201, 3143, 9494, 806, 10921, 10301, 7670, 2498, 1019, 4518, 10397, 11070, 852, 7765, 4529, 3745, 3951, 3760, 7726, 763, 5299, 3334, 2943, 7713, 1074, 5771, 3876, 9036, 6641, 9147, 5994, 1964, 6075, 7214, 6884, 6158, 1180, 3522, 8455, 7746, 6129, 7500, 3582, 9623, 9968, 3966, 9972, 4600, 3980, 6124, 3105, 4504, 4654, 7884, 4514, 2040, 5821, 959, 1655, 1835, 414, 5132, 8184, 1630, 1371, 4366, 5139, 1504, 5030, 4936, 9693, 2147, 3532, 527, 6797, 4259, 9618, 4736, 5549, 2076, 10652, 10544, 5570, 8281, 4449, 6781, 8159, 2622, 519, 6996, 6575, 1475, 9935, 5478, 463, 10262, 5979, 3419, 2956, 11102, 6399, 10421, 7666, 7889, 2785, 7695, 735, 449, 6718, 9613, 5832, 4502, 2421, 1075, 1282, 8017, 873, 4334, 6684, 10934, 2126, 4535, 10008], [3944, 4802, 10971, 7986, 3411, 5991, 4920, 9093, 3796, 1596, 3234, 6260, 5741, 5568, 5716, 141, 4585, 5430, 7069, 3, 520, 1211, 931, 137, 6784, 1352, 4049, 5736, 7451, 1975, 4678, 7429, 9012, 10377, 1483, 7622, 9962, 5012, 2678, 2578, 7216, 10328, 6456, 600, 4867, 6638, 4, 7640, 8749, 5796, 3791, 6610, 8208, 7835, 8295, 2781, 2947, 451, 284, 8188, 6168, 9131, 1008, 10373, 9637, 9402, 4466, 9535, 11046, 476, 7853, 9665, 2403, 1233, 3318, 3263, 527, 4238, 8439, 799, 7679, 5497, 3134, 1841, 4519, 3286, 8202, 7161, 9857, 5002, 4251, 10531, 9054, 9506, 10806, 8713, 10704, 2249, 4499, 9018, 4716, 3292, 4005, 7012, 9588, 7908, 1576, 10647, 5881, 1142, 803, 10670, 5, 1605, 5414, 1012, 2126, 4508, 2271, 9373, 275, 1904, 7629, 1260, 2840, 9020, 9334, 1660, 945, 977, 2421, 6519, 3623, 3875, 2091, 2315, 5249, 10814, 4907, 786, 9301, 4202, 4168, 6379, 7275, 4146, 6399, 1621, 9924, 1324, 4979, 9938, 9789, 1093, 4370, 7150, 4880, 1471, 652, 2851, 4378, 8087, 3300, 4299, 4235, 4434, 6598, 6721, 1050, 1622, 4110, 3369, 1814, 9364, 1434, 1432, 3782, 3426, 3268, 5372, 9860, 9785, 8699, 7006, 4200, 5370, 2744, 8072, 4589, 6793, 6538, 4559, 3980, 2839, 6187, 3677, 10092, 1320, 6117, 6179, 1859, 4689, 689, 2576, 176, 8969, 1046, 8975, 2624, 5259, 4916, 6728, 6945, 5793, 6047, 5811, 3011, 4212, 9618, 1513, 9147, 4001, 3506, 4531, 8643, 10093, 10237, 5964, 2076, 7785, 5944, 8522, 6803, 10659, 1419, 8163, 5289, 4006, 4226, 9116, 7282, 7907, 3157, 534, 1167, 593, 4969, 6173, 5895, 4597, 10636, 9669, 7596, 1196, 5513, 5381, 2150, 7373, 9023, 4373, 3294, 4588, 4703, 5825, 5396, 1174, 3128, 1317, 5629, 871, 5697, 8581, 8702, 283, 9768, 1625, 383, 9042, 3069, 2778, 768, 1231, 3392, 6530, 5771, 9461, 5214, 4726, 1890, 540, 6908, 4650, 10922, 10658, 1716, 2852, 742, 7140, 8030, 10368, 8656, 6837, 9335, 4331, 4375, 5498, 1179, 1246, 100, 733, 6990, 2707, 6404, 11064, 9489, 6327, 8534, 1581, 6943, 6627, 1629, 817, 6346, 4099, 4071, 3007, 3899, 4198, 2316, 7120, 4313, 10882, 637, 351, 9563, 5141, 3850, 2561, 7537, 7046, 2, 1107, 828, 10174, 8297, 11060, 10103, 6411, 253, 475, 4737, 6000, 9792, 2598, 3301, 2169, 3004, 1897, 4062, 10439, 1736, 790, 826, 1444, 7546, 5911, 4756, 1099, 10738, 7887, 10200, 5994, 6649, 5889, 2461, 840, 7554, 2057, 4044, 1665, 2527, 1282], [10090, 5232, 11060, 11109, 4023, 7251, 4184, 1575, 2421, 3908, 9130, 4900, 7962, 5596, 2634, 941, 5849, 8997, 8200, 10163, 2076, 4187, 5957, 268, 6834, 5539, 2693, 1777, 4558, 70, 3315, 2711, 10241, 8428, 2482, 4422, 8292, 9720, 5591, 5585, 7416, 7588, 7556, 1024, 4259, 1102, 8256, 10308, 595, 7658, 1219, 1546, 9782, 4221, 3676, 8568, 3426, 699, 10597, 11079, 5848, 4015, 8530, 9479, 1209, 6292, 3572, 9863, 7569, 141, 9649, 871, 9662, 10329, 2669, 5081, 2823, 50, 8980, 6437, 8632, 1085, 6050, 7737, 6433, 6608, 3132, 10957, 4870, 4720, 841, 2992, 10507, 6103, 5460, 7746, 2485, 5124, 2675, 6776, 154, 7429, 9044, 2828, 5236, 8410, 7324, 698, 9807, 4071, 489, 10715, 10485, 9962, 9619, 335, 11000, 1155, 468, 2097, 2850, 446, 8257, 10410, 7263, 10948, 7441, 4298, 4933, 5143, 4340, 10162, 3965, 2271, 8770, 5349, 5470, 7452, 9469, 7239, 8886, 5816, 5739, 440, 7947, 8457, 10234, 7361, 7877, 4429, 6094, 8373, 4724, 7189, 9667, 7372, 6837, 8488, 5505, 5073, 8581, 4964, 8512, 10259, 3015, 438, 7249, 9759, 5426, 8585, 8618, 4690, 2046, 3629, 3271, 10110, 4504, 4630, 937, 1859, 4338, 2318, 2148, 3660, 572, 9986, 1342, 7592, 4204, 1965, 7362]]
# counter = Counter()
# for cur_list in abnormal_batches:
#     counter.update(set(cur_list))
# fishy_examples = [elem for elem, count in counter.items() if count >= 3]
# print(fishy_examples)

# all_data = MyDataset([',', '\t'], ['data/kaggle spam.csv', 'data/UC Irvine collection/SMSSpamCollection'])
# for i in fishy_examples:
#     print(f'{all_data[i][2]}, {all_data[i][3]}')