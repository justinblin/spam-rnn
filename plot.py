import matplotlib.pyplot as plt
from collections import Counter
from dataset import MyDataset
import torch

# currently saved 750 4x model
train_losses = [0.022345484437600923, 0.026480530893343582, 0.01944205602332298, 0.023806724358391294, 0.005586442912658162, 0.009612390752098594, 0.014416106479483147, 0.013160334959084793, 0.02720217224369669, 0.014795571752856092, 0.02179763377174897, 0.02212597146970619, 0.02237122586047198, 0.01644812394261954, 0.02777380113634988, 0.01944089667526544, 0.011415038501525306, 0.021339162409637086, 0.02427121392027359, 0.030791511481584283, 0.01926000726356135, 0.0045501655077794965, 0.0063028407491949896, 0.007494712598819509, 0.013665074060476247, 0.012651395054018438, 0.013038220931302118, 0.01942223053360112, 0.019269891839493455, 0.018302171215988344, 0.021041674330878217, 0.01010964210626162, 0.008168102389747188, 0.01865463138782241, 0.0231796243044968, 0.009659112398318823, 0.022950205181345787, 0.013418343996358098, 0.010610035095823943, 0.005842676767897299]
test_losses = [0.03370918068002048, 0.02462364675827468, 0.023595734346578575, 0.011604223704968818, 0.0054730977387838055, 0.009631575919178983, 0.014300687060969482, 0.012373332339280835, 0.03193288485016474, 0.012898936465597654, 0.021365962045596286, 0.02203046446659616, 0.022176821717732515, 0.013390631417705188, 0.027587984511093334, 0.017936447440239832, 0.011313758146452914, 0.02112304935233351, 0.016895047894480205, 0.030374025203116804, 0.013344993629547181, 0.004468048977030343, 0.009248063268101559, 0.005330645974590309, 0.01345665977300697, 0.012500872652814262, 0.012882488217789678, 0.025172699081446374, 0.024985037614475705, 0.014093322317978383, 0.020730708569815265, 0.004586764700480838, 0.007814113325515713, 0.018338344321870873, 0.022593036469800834, 0.009052954356831129, 0.022746714385283445, 0.012202089921933569, 0.010514265225537228, 0.00584266869944012]      
learning_rates = [0.002, 0.001, 0.002, 0.002, 6.25e-05, 0.00025, 0.002, 0.00025, 0.002, 0.001, 6.25e-05, 6.25e-05, 0.002, 6.25e-05, 6.25e-05, 0.0005, 6.25e-05, 6.25e-05, 0.002, 0.002, 0.002, 6.25e-05, 0.002, 0.0005, 6.25e-05, 6.25e-05, 6.25e-05, 0.002, 0.002, 0.00025, 6.25e-05, 0.001, 0.001, 0.002, 0.000125, 0.0005, 6.25e-05, 0.0005, 6.25e-05, 6.25e-05]

# all_data, flawed
# tensor([[0.8082, 0.0582],
#         [0.0576, 0.0761]], device='cuda:0')
# 88.42152466367713% correct
# precision: 0.5691275000572205
# recall: 0.5664662718772888
# f1 score: 0.5677937865257263

train_losses = [0.10867800964619555, 0.10662580561703495, 0.11392093872335769, 0.1056830757562237, 0.09655092761809404, 0.0990807995442406, 0.10226751208931009, 0.08856825098773025, 0.10351781430523088, 0.09304712310669913, 0.1038565468558106, 0.11171230462346847, 0.10658016777656974, 0.09139975341856175, 0.10107509305446014, 0.09426422248638813, 0.10098786135735843, 0.09980890743960893, 0.09213230549655872, 0.10618019086359803]
test_metrics = ([0.11091166698644236, 0.09833170850205683, 0.10531309787498733, 0.10382128023164307, 0.09635864091675617, 0.09594697088644069, 0.0998013265291682, 0.09149891415899894, 0.10299612904283187, 0.09214397374572993, 0.10444506935592161, 0.10995309399817611, 0.09619151262105792, 0.0898648763978658, 0.09678620975264297, 0.09283570919798176, 0.10346874590110486, 0.09842036408694109, 0.08927743758241367, 0.10340232800938384], [86.50224215246637, 85.47085201793722, 86.50224215246637, 86.50224215246637, 86.41255605381166, 86.00896860986548, 86.23318385650224, 86.41255605381166, 86.32286995515695, 85.87443946188341, 86.41255605381166, 86.09865470852019, 86.09865470852019, 86.63677130044843, 85.91928251121077, 85.91928251121077, 86.27802690582959, 86.00896860986548, 86.23318385650224, 85.96412556053812], [0.49829354882240295, 0.46341463923454285, 0.4983050525188446, 0.4983050525188446, 0.49494948983192444, 0.48064514994621277, 0.4883720576763153, 0.49494948983192444, 0.49163880944252014, 0.4760383367538452, 0.49494948983192444, 0.48366016149520874, 0.48366016149520874, 0.5034722685813904, 0.477419376373291, 0.477419376373291, 0.49000000953674316, 0.4803921580314636, 0.4883720576763153, 0.4788273274898529], [0.4866666793823242, 0.5066666603088379, 0.49000000953674316, 0.49000000953674316, 0.49000000953674316, 0.49666666984558105, 0.49000000953674316, 0.49000000953674316, 0.49000000953674316, 0.49666666984558105, 0.49000000953674316, 0.4933333694934845, 0.4933333694934845, 0.48333337903022766, 0.4933333694934845, 0.4933333694934845, 0.49000000953674316, 0.49000000953674316, 0.49000000953674316, 0.49000000953674316], [0.4924114899645022, 0.484076432995276, 0.49411763606388914, 0.49411763606388914, 0.4924623138410707, 0.4885245858421571, 0.4891846791998834, 0.4924623138410707, 0.4908180415398933, 0.4861337688821577, 0.4924623138410707, 0.48844887830266626, 0.48844887830266626, 0.4931973249452504, 0.4852459302546996, 0.4852459302546996, 0.49000000953674316, 0.48514852012188897, 0.4891846791998834, 0.4843492458760797])
learning_rates = [0.001, 0.001, 0.001, 0.0005, 0.00025, 0.000125, 6.25e-05, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 0.00025, 0.00025, 0.00025, 0.0005, 6.25e-05, 6.25e-05, 6.25e-05, 0.00025, 6.25e-05]

# no tuning
# tensor([[0.7937, 0.0686],
#         [0.0717, 0.0659]], device='cuda:0')
# 85.96412556053812% correct
# precision: 0.4788273274898529
# recall: 0.49000000953674316
# f1 score: 0.4843492458760797

# yes tuning
# tensor([[0.8054, 0.0722],
#         [0.0601, 0.0623]], device='cuda:0')
# 86.7713004484305% correct
# precision: 0.5091575384140015
# recall: 0.4633333683013916
# f1 score: 0.48516582651902423

# inverse of tuning
# tensor([[0.7807, 0.0655],
#         [0.0848, 0.0691]], device='cuda:0')
# 84.97757847533633% correct
# precision: 0.44897958636283875
# recall: 0.5133333802223206
# f1 score: 0.4790046829282944

plt.figure()
plt.plot(train_losses)
for i in range(len(test_metrics)):
    if i == 0:
        plt.plot(test_metrics[i])
plt.plot(learning_rates)
plt.legend(['train loss', 'test loss', 
            # 'test accuracy', 
            # 'test precision', 'test recall', 'test f1',
            'learning rates'])
plt.show()

# abnormal_batches = [[10427, 7685, 10288, 8303, 2707, 6793, 6883, 11040, 6914, 2031, 2462, 8047, 6209, 1774, 10049, 9738, 7025, 7250, 799, 10314, 579, 2967, 547, 2750, 2073, 3427, 6236, 1882, 3849, 4799, 9301, 9935, 3292, 4457, 9553, 7013, 3691, 6685, 8997, 3720, 8448, 3304, 3983, 4678, 8478, 10660, 8399, 3269, 38, 2961, 9059, 9071, 2744, 7665, 2421, 4536, 2912, 5641, 6852, 7230, 7597, 2810, 2451, 3283, 2313, 1859, 581, 3970, 3276, 4508, 6034, 1767, 6016, 1782, 435, 10805, 10647, 10204, 5331, 476, 9772, 10779, 5696, 9331, 9514, 275, 8021, 5182, 3424, 520, 10163, 9123, 10655, 10588, 6314, 5913, 5735, 1627, 8382, 2344, 637, 8306, 8394, 1468, 2345, 7204, 8456, 3088, 4847, 5385, 1122, 6353, 5816, 2977, 332, 893, 5349, 1091, 4289, 10308, 1242, 9986, 1889, 9132, 7415, 10073, 7267], [6683, 941, 1099, 2672, 7100, 10983, 4762, 10964, 9782, 10026, 6583, 7362, 7667, 6600, 8090, 7637, 799, 5835, 4747, 10501, 566, 7485, 9770, 6893, 72, 8671, 10150, 7102, 5232, 10440, 8798, 1306, 9759, 7374, 5124, 7514, 7401, 2598, 10582, 3423, 2748, 5196, 7927, 10839, 2117, 11027, 6425, 805, 7068, 4892, 9462, 2982, 850, 2881, 8240, 9255, 9775, 3712, 7605, 4667, 6476, 4428, 10445, 650, 2756, 9870, 7695, 1859, 2924, 5081, 4341, 9867, 9448, 2072, 4238, 6211, 7826, 3224, 8616, 1716, 14, 9500, 8267, 4604, 7890, 5464, 1380, 2376, 8159, 2482, 8883, 177, 763, 4584, 1769, 8557, 12, 8097, 5591, 3485, 2135, 10951, 3765, 982, 9326, 3983, 10155, 4395, 10924, 8323, 257, 1575, 10464, 1367, 4293, 5145, 10699, 10564, 1811, 6932, 4127, 4219, 1783, 4208, 1432, 8668, 4171, 7676, 9648, 1312, 7887, 11118, 4536, 9533, 1093, 8639, 11060, 580, 979, 8836, 4570, 2850, 4545, 1199, 100, 9993, 2226, 3703, 3426, 6638, 2869, 833, 7777, 6307, 3890, 10508, 5205, 1942, 6399, 5247, 9059, 4630, 9857, 5170, 8478, 7889, 3678, 5570, 2671, 5360, 1690, 4145, 4461, 6582, 2658, 7727, 116, 10911, 8299, 8208, 8350, 7745, 2818, 9566, 3681, 8597, 6062, 9318, 2269, 6091, 1337, 9733, 6894, 3435, 2292, 9000, 7383, 11005, 2310, 5683, 1319, 1071, 397, 2951, 9033, 3658, 6956, 5634, 2421, 8132, 2806, 5980, 4274, 4576, 9637, 706, 3858, 1615, 11116, 4908, 1209, 6342, 6339, 6626, 1595, 10942, 9933, 9602, 4836, 9354, 440, 4376, 9044, 6883, 6694, 10606, 6096, 10936, 6313, 7032, 8346, 6030, 5671, 166, 1895, 8953, 4512, 7150, 7201, 2628, 1409, 6200, 10180, 8768], [10025, 4577, 5460, 8211, 3502, 4838, 2143, 2292, 9424, 8493, 7474, 8686, 9516, 3864, 5593, 833, 675, 5911, 3925, 1165, 1120, 7369, 5723, 7849, 4440, 6963, 7627, 1156, 1523, 2016, 9289, 2097, 5453, 4435, 9136, 1409, 9463, 2881, 10219, 4542, 8119, 2391, 3765, 5040, 6206, 8262, 6437, 8051, 9118, 8798, 1859, 9662, 3807, 10492, 5909, 9438, 8353, 2246, 9488, 3723, 9472, 865, 8212, 8886, 9301, 7704, 5544, 570, 9588, 7955, 6531, 2897, 5298, 2811, 9334, 1987, 6370, 10082, 1821, 13, 307, 6640, 507, 4409, 10525, 2864, 8120, 805, 3681, 10810, 9647, 1407, 3191, 3393, 841, 1774, 7540, 720, 5145, 4003, 10075, 3100, 4462, 7170, 9745, 3513, 10453, 4737, 8360, 1897, 4001, 6228, 2899, 7481, 7076, 1174, 1309, 5548, 1913, 4367, 7464, 8134, 9821, 11060, 7038, 7194, 7904, 3575, 8439, 10920, 5119, 8278, 8555, 6093, 4563, 5528, 939, 10150, 2852, 7936, 10259, 4589, 5431, 1439, 10172, 4156, 6805, 9205, 8305, 10283, 1500, 9383, 3768, 1376, 8768, 5265, 1124, 9614, 6630, 2211, 10305, 6185, 2932, 8586, 3536, 5888, 7215, 7055, 4171, 3551, 6371, 1584, 4904, 9918, 3818, 37, 1399, 10885, 6757, 2250, 4558, 10988, 7150, 3094, 3435, 580, 10482, 4857, 2521, 9916, 673, 9864, 2754, 9163, 6183, 7201, 3143, 9494, 806, 10921, 10301, 7670, 2498, 1019, 4518, 10397, 11070, 852, 7765, 4529, 3745, 3951, 3760, 7726, 763, 5299, 3334, 2943, 7713, 1074, 5771, 3876, 9036, 6641, 9147, 5994, 1964, 6075, 7214, 6884, 6158, 1180, 3522, 8455, 7746, 6129, 7500, 3582, 9623, 9968, 3966, 9972, 4600, 3980, 6124, 3105, 4504, 4654, 7884, 4514, 2040, 5821, 959, 1655, 1835, 414, 5132, 8184, 1630, 1371, 4366, 5139, 1504, 5030, 4936, 9693, 2147, 3532, 527, 6797, 4259, 9618, 4736, 5549, 2076, 10652, 10544, 5570, 8281, 4449, 6781, 8159, 2622, 519, 6996, 6575, 1475, 9935, 5478, 463, 10262, 5979, 3419, 2956, 11102, 6399, 10421, 7666, 7889, 2785, 7695, 735, 449, 6718, 9613, 5832, 4502, 2421, 1075, 1282, 8017, 873, 4334, 6684, 10934, 2126, 4535, 10008], [3944, 4802, 10971, 7986, 3411, 5991, 4920, 9093, 3796, 1596, 3234, 6260, 5741, 5568, 5716, 141, 4585, 5430, 7069, 3, 520, 1211, 931, 137, 6784, 1352, 4049, 5736, 7451, 1975, 4678, 7429, 9012, 10377, 1483, 7622, 9962, 5012, 2678, 2578, 7216, 10328, 6456, 600, 4867, 6638, 4, 7640, 8749, 5796, 3791, 6610, 8208, 7835, 8295, 2781, 2947, 451, 284, 8188, 6168, 9131, 1008, 10373, 9637, 9402, 4466, 9535, 11046, 476, 7853, 9665, 2403, 1233, 3318, 3263, 527, 4238, 8439, 799, 7679, 5497, 3134, 1841, 4519, 3286, 8202, 7161, 9857, 5002, 4251, 10531, 9054, 9506, 10806, 8713, 10704, 2249, 4499, 9018, 4716, 3292, 4005, 7012, 9588, 7908, 1576, 10647, 5881, 1142, 803, 10670, 5, 1605, 5414, 1012, 2126, 4508, 2271, 9373, 275, 1904, 7629, 1260, 2840, 9020, 9334, 1660, 945, 977, 2421, 6519, 3623, 3875, 2091, 2315, 5249, 10814, 4907, 786, 9301, 4202, 4168, 6379, 7275, 4146, 6399, 1621, 9924, 1324, 4979, 9938, 9789, 1093, 4370, 7150, 4880, 1471, 652, 2851, 4378, 8087, 3300, 4299, 4235, 4434, 6598, 6721, 1050, 1622, 4110, 3369, 1814, 9364, 1434, 1432, 3782, 3426, 3268, 5372, 9860, 9785, 8699, 7006, 4200, 5370, 2744, 8072, 4589, 6793, 6538, 4559, 3980, 2839, 6187, 3677, 10092, 1320, 6117, 6179, 1859, 4689, 689, 2576, 176, 8969, 1046, 8975, 2624, 5259, 4916, 6728, 6945, 5793, 6047, 5811, 3011, 4212, 9618, 1513, 9147, 4001, 3506, 4531, 8643, 10093, 10237, 5964, 2076, 7785, 5944, 8522, 6803, 10659, 1419, 8163, 5289, 4006, 4226, 9116, 7282, 7907, 3157, 534, 1167, 593, 4969, 6173, 5895, 4597, 10636, 9669, 7596, 1196, 5513, 5381, 2150, 7373, 9023, 4373, 3294, 4588, 4703, 5825, 5396, 1174, 3128, 1317, 5629, 871, 5697, 8581, 8702, 283, 9768, 1625, 383, 9042, 3069, 2778, 768, 1231, 3392, 6530, 5771, 9461, 5214, 4726, 1890, 540, 6908, 4650, 10922, 10658, 1716, 2852, 742, 7140, 8030, 10368, 8656, 6837, 9335, 4331, 4375, 5498, 1179, 1246, 100, 733, 6990, 2707, 6404, 11064, 9489, 6327, 8534, 1581, 6943, 6627, 1629, 817, 6346, 4099, 4071, 3007, 3899, 4198, 2316, 7120, 4313, 10882, 637, 351, 9563, 5141, 3850, 2561, 7537, 7046, 2, 1107, 828, 10174, 8297, 11060, 10103, 6411, 253, 475, 4737, 6000, 9792, 2598, 3301, 2169, 3004, 1897, 4062, 10439, 1736, 790, 826, 1444, 7546, 5911, 4756, 1099, 10738, 7887, 10200, 5994, 6649, 5889, 2461, 840, 7554, 2057, 4044, 1665, 2527, 1282], [10090, 5232, 11060, 11109, 4023, 7251, 4184, 1575, 2421, 3908, 9130, 4900, 7962, 5596, 2634, 941, 5849, 8997, 8200, 10163, 2076, 4187, 5957, 268, 6834, 5539, 2693, 1777, 4558, 70, 3315, 2711, 10241, 8428, 2482, 4422, 8292, 9720, 5591, 5585, 7416, 7588, 7556, 1024, 4259, 1102, 8256, 10308, 595, 7658, 1219, 1546, 9782, 4221, 3676, 8568, 3426, 699, 10597, 11079, 5848, 4015, 8530, 9479, 1209, 6292, 3572, 9863, 7569, 141, 9649, 871, 9662, 10329, 2669, 5081, 2823, 50, 8980, 6437, 8632, 1085, 6050, 7737, 6433, 6608, 3132, 10957, 4870, 4720, 841, 2992, 10507, 6103, 5460, 7746, 2485, 5124, 2675, 6776, 154, 7429, 9044, 2828, 5236, 8410, 7324, 698, 9807, 4071, 489, 10715, 10485, 9962, 9619, 335, 11000, 1155, 468, 2097, 2850, 446, 8257, 10410, 7263, 10948, 7441, 4298, 4933, 5143, 4340, 10162, 3965, 2271, 8770, 5349, 5470, 7452, 9469, 7239, 8886, 5816, 5739, 440, 7947, 8457, 10234, 7361, 7877, 4429, 6094, 8373, 4724, 7189, 9667, 7372, 6837, 8488, 5505, 5073, 8581, 4964, 8512, 10259, 3015, 438, 7249, 9759, 5426, 8585, 8618, 4690, 2046, 3629, 3271, 10110, 4504, 4630, 937, 1859, 4338, 2318, 2148, 3660, 572, 9986, 1342, 7592, 4204, 1965, 7362]]
# counter = Counter()
# for cur_list in abnormal_batches:
#     counter.update(set(cur_list))
# fishy_examples = [elem for elem, count in counter.items() if count >= 3]
# print(fishy_examples)

# all_data = MyDataset([',', '\t'], ['data/kaggle spam.csv', 'data/UC Irvine collection/SMSSpamCollection'])
# for i in fishy_examples:
#     print(f'{all_data[i][2]}, {all_data[i][3]}')