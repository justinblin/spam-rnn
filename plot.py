import matplotlib.pyplot as plt

# currently saved 750 4x model
train_losses = [0.02601871023156415, 0.023295618714822035, 0.04235645612844475, 0.02954874277632798, 0.03963506104912809, 0.028603407060468402, 0.02974474202758517, 0.025500778675817813, 0.029823847293141695, 0.026610830694934896, 0.026247990581825708, 0.021136844166833046, 0.027059860968353118, 0.034336765576991196, 0.021273905044693984, 0.022353183995829543, 0.020896351159117928, 0.026373881040583566, 0.021008479920325115, 0.024548376484446792, 0.02356158466896906, 0.020784853640892477, 0.01935440353511544, 0.03289250876357383, 0.027698300476457507, 0.028928503744018678, 0.039268308792477544, 0.03428128489355307, 0.025262625213520275, 0.026198603588373698, 0.017150296854982976, 0.016261903213520792, 0.020939261782346282, 0.028523254769064643, 0.016088862309672564, 0.01784185002587103, 0.02129272946589508, 0.06536604884908506, 0.02456898591622567, 0.024366784053123793]
test_losses = [0.026405537443260102, 0.022980391192367614, 0.025995500854439465, 0.025659312547724855, 0.03051453021674842, 0.021424325586697567, 0.027128609996438735, 0.02282827102257611, 0.029804152705672948, 0.026464053427881308, 0.025973958540822566, 0.02094145470051175, 0.026920612945267576, 0.02814572901695567, 0.01878019764567709, 0.022026389051037334, 0.020621621484961297, 0.026210474307790783, 0.020673151141551334, 0.024511686235191184, 0.023417739635121483, 0.022707206256610567, 0.01846760048633505, 0.02980347969587556, 0.02703260471791965, 0.019638289330097237, 0.03109593286701989, 0.02129342861517947, 0.0247452531823745, 0.02162353471494825, 0.016605660614788284, 0.016061429579824166, 0.020541473561102544, 0.02656877454371241, 0.015916331119718737, 0.017462364818623947, 0.020950620701274215, 0.04079528255468843, 0.01800048413987882, 0.019847304872655368]
learning_rates = [0.001, 6.25e-05, 0.004, 0.0005, 0.004, 0.002, 6.25e-05, 0.001, 0.00025, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 0.002, 0.001, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 0.001, 0.00025, 0.002, 0.000125, 0.004, 0.004, 0.004, 6.25e-05, 0.001, 6.25e-05, 6.25e-05, 6.25e-05, 0.0005, 6.25e-05, 6.25e-05, 6.25e-05, 0.008, 0.001, 0.001]

# tensor([[0.8089, 0.0591],
#         [0.0569, 0.0752]], device='cuda:0')
# 88.4035874439462% correct
# precision: 0.5692934393882751
# recall: 0.5597862601280212
# f1 score: 0.5644997954368591

# my_model copy
train_losses = [0.022345484437600923, 0.026480530893343582, 0.01944205602332298, 0.023806724358391294, 0.005586442912658162, 0.009612390752098594, 0.014416106479483147, 0.013160334959084793, 0.02720217224369669, 0.014795571752856092, 0.02179763377174897, 0.02212597146970619, 0.02237122586047198, 0.01644812394261954, 0.02777380113634988, 0.01944089667526544, 0.011415038501525306, 0.021339162409637086, 0.02427121392027359, 0.030791511481584283, 0.01926000726356135, 0.0045501655077794965, 0.0063028407491949896, 0.007494712598819509, 0.013665074060476247, 0.012651395054018438, 0.013038220931302118, 0.01942223053360112, 0.019269891839493455, 0.018302171215988344, 0.021041674330878217, 0.01010964210626162, 0.008168102389747188, 0.01865463138782241, 0.0231796243044968, 0.009659112398318823, 0.022950205181345787, 0.013418343996358098, 0.010610035095823943, 0.005842676767897299]
test_losses = [0.03370918068002048, 0.02462364675827468, 0.023595734346578575, 0.011604223704968818, 0.0054730977387838055, 0.009631575919178983, 0.014300687060969482, 0.012373332339280835, 0.03193288485016474, 0.012898936465597654, 0.021365962045596286, 0.02203046446659616, 0.022176821717732515, 0.013390631417705188, 0.027587984511093334, 0.017936447440239832, 0.011313758146452914, 0.02112304935233351, 0.016895047894480205, 0.030374025203116804, 0.013344993629547181, 0.004468048977030343, 0.009248063268101559, 0.005330645974590309, 0.01345665977300697, 0.012500872652814262, 0.012882488217789678, 0.025172699081446374, 0.024985037614475705, 0.014093322317978383, 0.020730708569815265, 0.004586764700480838, 0.007814113325515713, 0.018338344321870873, 0.022593036469800834, 0.009052954356831129, 0.022746714385283445, 0.012202089921933569, 0.010514265225537228, 0.00584266869944012]      
learning_rates = [0.002, 0.001, 0.002, 0.002, 6.25e-05, 0.00025, 0.002, 0.00025, 0.002, 0.001, 6.25e-05, 6.25e-05, 0.002, 6.25e-05, 6.25e-05, 0.0005, 6.25e-05, 6.25e-05, 0.002, 0.002, 0.002, 6.25e-05, 0.002, 0.0005, 6.25e-05, 6.25e-05, 6.25e-05, 0.002, 0.002, 0.00025, 6.25e-05, 0.001, 0.001, 0.002, 0.000125, 0.0005, 6.25e-05, 0.0005, 6.25e-05, 6.25e-05]

# 0, 2-3, 6, 8, 12, 18-20, 22, 27-28, 33

# tensor([[0.8082, 0.0582],
#         [0.0576, 0.0761]], device='cuda:0')
# 88.42152466367713% correct
# precision: 0.5691275000572205
# recall: 0.5664662718772888
# f1 score: 0.5677937865257263

# start epoch 0, learning rate: 0.0005
# 0% complete, loss for current batch: 0.00023146932653617114, 23/64 spam
# 10% complete, loss for current batch: 9.360985859530047e-05, 22/64 spam
# 20% complete, loss for current batch: 6.908812065375969e-05, 29/64 spam
# 30% complete, loss for current batch: 0.0007954557659104466, 21/64 spam
# abnormal batch 18 loss: 0.12208405137062073, 30/64 spam, batch = [
#   7514  4110  7793   517  7166  3422  8462  2379  6456  9001  9108  3864
#   6415  1184  2746  5254  5399  1320 10046 10877  2576  8318  6807 10016
#   6533  6739  9088  6531  5078  5448  5571   494 10927   426  5040  8106
#   4013  7015   667 10552  2349  7094  9308  5557   485  4174 11034  4071
#  10813  6094  7357  1271  1917  2055 10028  4312   806  7250   422  2970
#    405  3291  1581  3795]
# 40% complete, loss for current batch: 0.012036893516778946, 25/64 spam
# 51% complete, loss for current batch: 0.00012135436554672197, 27/64 spam
# 61% complete, loss for current batch: 0.0002951546339318156, 27/64 spam
# 71% complete, loss for current batch: 0.0004996195202693343, 28/64 spam
# abnormal batch 38 loss: 0.12426628917455673, 19/64 spam, batch = [ 
#   6232  7358 10880  1983   868  4678  6439  8094  6211  5819  2742  2176
#   2898 10021  2967  7350 10490  2091  2949  7769  1764  4133  4014  6419
#   8265  8869  3303  4067  1761  3970   795  1471  7430   808  4933  8387
#   5258  2511  7662  9626  2953  2881 10043  6079 10025   608  8468  6662
#    594  3907  9143  7110  1506  6600  9342  8447  4539    54 10676  8476
#   8585  1630  1889   935]
# 81% complete, loss for current batch: 0.0009874801617115736, 30/64 spam
# 91% complete, loss for current batch: 0.00011071695813110896, 17/64 spam

# FINISH EPOCH 0: training average batch loss = 0.006534994160956104, testing loss = 0.00600319942198595

# start epoch 1, learning rate: 6.25e-05
# 0% complete, loss for current batch: 0.00013111312000546604, 27/64 spam
# 10% complete, loss for current batch: 0.0005798085476271808, 29/64 spam
# abnormal batch 7 loss: 0.12131363153457642, 23/64 spam, batch = [10021  6441  5381  7661  1107  2143  2002 10277 10490  8156 10464  9292
#  10955  7937  6781  5306  6886  2220  6168  8923  5735  8165  3483  3001
#   8065  3703  3083   507  2275  5205  2369   142  9649  1700  7149  9299
#   5470  2810  5805  6128  8269  9064  7728   781 10342   131  9944 11134
#   2806   420  6533  9902  1781  3636  5881  8563  2063 10027  3352  8447
#   5309 10252  3529  5765]
# 20% complete, loss for current batch: 0.001397885731421411, 30/64 spam
# abnormal batch 14 loss: 0.05273919180035591, 26/64 spam, batch = [ 4411 10190  6409  4763  7361 10092   120  3387  8283  8432  7006  3974
#    415  7950 10838  5742  8623 10420 11111  2497  8188  6389  1391  1091
#   6039  7793   370  5658  2008  5799  9952  7120 10501  2239  5358    81
#   1074     7  9174  5407  9735  7155  3097 10169  4752   231  6124  2840
#  11005  5139  9873  4238  1316  9745  9285  5909  2883  2914  4283  8937
#   3012  7646  5282  3782]
# 30% complete, loss for current batch: 0.0009363180724903941, 31/64 spam
# abnormal batch 17 loss: 0.052904825657606125, 23/64 spam, batch = [ 7452 10946  3787  5764  5032 11046  7377  7563  4950  3425  1743 10279
#   6783  4437  3616  2011  2320  2947  8892  4171  9262  9566   961  8534
#   1489  6373  5696 10770   676  3233  6042  7564  4422  1582  7426  6515
#    298  2226  1528  2166  2675  1415  2825  1938  4189  6250  5081  5266
#   7826  1049   799  3833  8612  9151  5975  6869  1779  9763  6706  8820
#   8873  7094  3859  4513]
# 40% complete, loss for current batch: 0.0001036939793266356, 28/64 spam
# 51% complete, loss for current batch: 0.0022115916945040226, 23/64 spam
# 61% complete, loss for current batch: 0.00027596368454396725, 28/64 spam
# 71% complete, loss for current batch: 0.00441580219194293, 23/64 spam
# 81% complete, loss for current batch: 0.00030826887814328074, 26/64 spam
# abnormal batch 44 loss: 0.12094107128324963, 26/64 spam, batch = [  540  7036  8972  6568  7283  5916  5750  4149  5201  5182  5501  4667
#  10528  4460  2393  1338  7009  8438  1337  8991  8997    57  5946  5039
#    403  7133   858   870  3858  6307  9732   295 10067  8541  1075 10810
#   5387  5644  5557  5256  9266  5736   703  3250  7454  4857  5783  4485
#   4835  4865 10322  9009  2967  9000   749  8832   650  1093  6417  1124
#  10408   665  5460]
# 91% complete, loss for current batch: 0.00023882723753414456, 27/64 spam
    
# FINISH EPOCH 1: training average batch loss = 0.008932368609078697, testing loss = 0.008768062219219757
    
# start epoch 2, learning rate: 6.25e-05
# 0% complete, loss for current batch: 0.0005884605343453586, 22/64 spam
# abnormal batch 1 loss: 0.11809007823467255, 27/64 spam, batch = [ 7847  1092  9952 10667  8768  1075  4866  7506  4013  6389 10453  1321
#   9391 10956  8793  8884  6612  8999  8234  6718  9432  7897  4553  7310
#   6373  8215  2497  9334  7297  6406  8399  8558  2496  1830  7709  3554
#   2096    60  4184  8806   371   909  8654 10299 10177  7837  8995  1347
#   4039  7721  2967  9763  2189  5218 10274  5822  6546  1469  3077  9729
#   9888  1259  1498  7464]
# 10% complete, loss for current batch: 0.0006914683035574853, 31/64 spam
# 20% complete, loss for current batch: 0.00022749940399080515, 23/64 spam
# abnormal batch 12 loss: 0.13669483363628387, 25/64 spam, batch = [ 7204  5528  3937  4514   927  7429  3683  4009   228  3425  2552  7177
#   9972  9792  3140 10778   193   158  3469 10311 10428  3012  8498 10725
#   9464  7871 10155  1965  9177  8321   447 10813  4343  3053  7295  6342
#   2397  1736 10125  7664 10161 10779  3908  1036   403  6803  8536  3231
#   9180  2598  5374  4786  9325  2314  2822  2396  6649  1143  1797   675
#   1665  5521 10860  8431]
# 30% complete, loss for current batch: 8.908511517802253e-05, 22/64 spam
# abnormal batch 16 loss: 0.12878379225730896, 27/64 spam, batch = [ 4475  1523  7264  9116   882  7121  2069  7275  6511  8202  4597   585
#   1764 11027  9612  3636  3926   307  4337  5802  9072  2709  9170  5587
#   6747  3007  9649  3300  5227  7918  4022  2438  5605  4752  6940  7149
#   6664 10194  7467  3143  5994  1071  7540  4760  7406  2676  6577  6755
#   1137  4876   134  9926  2186  9161  8488  7840  5427  5228  6696  9354
#   2744  4699 10866  5733]
# 40% complete, loss for current batch: 0.00207119039259851, 20/64 spam
# 51% complete, loss for current batch: 0.00033157027792185545, 24/64 spam
# 61% complete, loss for current batch: 0.000923684798181057, 31/64 spam
# abnormal batch 32 loss: 0.08879432827234268, 27/64 spam, batch = [ 4274  6297  7996  8909  1987  2840  4313  7476  3257  8578  8961  5424
#   2593  2303  7469    45  7154  3573  6079  7355  7860  9250 10049  6411
#   8214 10110  9142  3125  5406  3291  2917   858 10940  2513  1124  2801
#   3914  1024  1425  6883  8544  5930  5669 10638  2531  5119   427  3899
#   7397  4583  9564  5570  6055  5860  3201  2765  7576  7737  3550  7065
#   1432  8820  3462   423]
# 71% complete, loss for current batch: 0.00019799594883807003, 17/64 spam
# 81% complete, loss for current batch: 0.019740968942642212, 26/64 spam
# 91% complete, loss for current batch: 0.00037554728369864205, 18/64 spam

# FINISH EPOCH 2: training average batch loss = 0.010682599970237914, testing loss = 0.010534867487261741

plt.figure()
plt.plot(train_losses)
plt.plot(test_losses)
plt.plot(learning_rates)
plt.legend(['train loss', 'test loss', 'learning rates'])
plt.show()