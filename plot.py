import matplotlib.pyplot as plt
from collections import Counter
from dataset import MyDataset
import torch

# currently saved 750 4x model
train_losses = [0.022345484437600923, 0.026480530893343582, 0.01944205602332298, 0.023806724358391294, 0.005586442912658162, 0.009612390752098594, 0.014416106479483147, 0.013160334959084793, 0.02720217224369669, 0.014795571752856092, 0.02179763377174897, 0.02212597146970619, 0.02237122586047198, 0.01644812394261954, 0.02777380113634988, 0.01944089667526544, 0.011415038501525306, 0.021339162409637086, 0.02427121392027359, 0.030791511481584283, 0.01926000726356135, 0.0045501655077794965, 0.0063028407491949896, 0.007494712598819509, 0.013665074060476247, 0.012651395054018438, 0.013038220931302118, 0.01942223053360112, 0.019269891839493455, 0.018302171215988344, 0.021041674330878217, 0.01010964210626162, 0.008168102389747188, 0.01865463138782241, 0.0231796243044968, 0.009659112398318823, 0.022950205181345787, 0.013418343996358098, 0.010610035095823943, 0.005842676767897299]
test_losses = [0.03370918068002048, 0.02462364675827468, 0.023595734346578575, 0.011604223704968818, 0.0054730977387838055, 0.009631575919178983, 0.014300687060969482, 0.012373332339280835, 0.03193288485016474, 0.012898936465597654, 0.021365962045596286, 0.02203046446659616, 0.022176821717732515, 0.013390631417705188, 0.027587984511093334, 0.017936447440239832, 0.011313758146452914, 0.02112304935233351, 0.016895047894480205, 0.030374025203116804, 0.013344993629547181, 0.004468048977030343, 0.009248063268101559, 0.005330645974590309, 0.01345665977300697, 0.012500872652814262, 0.012882488217789678, 0.025172699081446374, 0.024985037614475705, 0.014093322317978383, 0.020730708569815265, 0.004586764700480838, 0.007814113325515713, 0.018338344321870873, 0.022593036469800834, 0.009052954356831129, 0.022746714385283445, 0.012202089921933569, 0.010514265225537228, 0.00584266869944012]      
learning_rates = [0.002, 0.001, 0.002, 0.002, 6.25e-05, 0.00025, 0.002, 0.00025, 0.002, 0.001, 6.25e-05, 6.25e-05, 0.002, 6.25e-05, 6.25e-05, 0.0005, 6.25e-05, 6.25e-05, 0.002, 0.002, 0.002, 6.25e-05, 0.002, 0.0005, 6.25e-05, 6.25e-05, 6.25e-05, 0.002, 0.002, 0.00025, 6.25e-05, 0.001, 0.001, 0.002, 0.000125, 0.0005, 6.25e-05, 0.0005, 6.25e-05, 6.25e-05]

# all_data, flawed
# tensor([[0.8082, 0.0582],
#         [0.0576, 0.0761]], device='cuda:0')
# 88.42152466367713% correct
# precision: 0.5691275000572205
# recall: 0.5664662718772888
# f1 score: 0.5677937865257263

train_metrics = ([0.583895277933798, 0.6747976426546782, 0.5599636950416348, 0.4236041710778217, 0.40013288540458414, 0.41619990337215823, 0.3992069134780504, 0.40997869828349565, 0.3967822574289854, 0.3694350723308312, 0.3868414540386266, 0.3726116599832591, 0.3793714058098914, 0.3755137850347266, 0.3719380266753619, 0.352229952256872, 0.36840607078837234, 0.3541175016479051, 0.3774422417485443, 0.3523627286474815], [60.72652339935303, 61.137765645980835, 63.22823762893677, 68.06031465530396, 68.91706585884094, 68.06031465530396, 68.43728423118591, 68.33447217941284, 69.91089582443237, 69.1912293434143, 68.54009628295898, 69.43112015724182, 69.43111419677734, 69.63673830032349, 70.0479805469513, 69.9794352054596, 69.73954439163208, 69.91089582443237, 69.63673830032349, 70.04797458648682], [0.5414201617240906, 0.5292397737503052, 0.5896980166435242, 0.7130281925201416, 0.722129762172699, 0.6983606815338135, 0.7077175378799438, 0.7029220461845398, 0.7652329206466675, 0.7425044178962708, 0.7229965329170227, 0.7350000143051147, 0.7431034445762634, 0.7544170022010803, 0.7595155835151672, 0.7595819234848022, 0.7679558396339417, 0.7652329206466675, 0.7448979616165161, 0.7631579041481018], [0.15587733685970306, 0.30834755301475525, 0.2827938497066498, 0.34497445821762085, 0.3696763217449188, 0.3628619909286499, 0.3671209514141083, 0.3688245415687561, 0.3637137711048126, 0.3586030900478363, 0.3534923493862152, 0.37563884258270264, 0.3671209514141083, 0.3637137711048126, 0.3739352822303772, 0.37137988209724426, 0.35519590973854065, 0.3637137711048126, 0.3730835020542145, 0.37052810192108154], [0.24206348969588132, 0.3896663284633563, 0.38226825652508273, 0.46498279420567024, 0.4890140808606804, 0.47757846465839005, 0.4834548416843207, 0.4837988838127516, 0.4930715566563143, 0.48363012096596913, 0.47482839297346957, 0.4971815148724456, 0.4914481154570212, 0.49080458101491825, 0.5011415715727598, 0.49885583013443185, 0.4857309321740452, 0.4930715566563143, 0.49716233984857167, 0.49885320657792553])
test_metrics = ([0.5866823942497568, 0.4978128713809515, 0.4773903726256248, 0.43391218811808335, 0.39423916644684776, 0.4256134974989419, 0.3968346894733489, 0.39160705037451016, 0.39917453362499145, 0.3613838291347149, 0.3837225959270555, 0.36139977047203153, 0.3686186392927187, 0.3610810226175273, 0.3453804460669732, 0.3564519412748728, 0.34822716869881765, 0.350260381789311, 0.3687846637989753, 0.35742214162691943], [87.2914731502533, 82.46891498565674, 85.89626550674438, 83.34850072860718, 81.58932328224182, 84.07642841339111, 82.49924182891846, 82.16560482978821, 84.25841331481934, 84.74370837211609, 84.77403521537781, 83.89444947242737, 84.89536046981812, 84.44040417671204, 84.50106382369995, 84.86502766609192, 84.53139662742615, 84.65271592140198, 85.16833186149597, 84.86502766609192], [0.453125, 0.30403801798820496, 0.3839285969734192, 0.3349514305591583, 0.3155893385410309, 0.3549222946166992, 0.32700422406196594, 0.32464930415153503, 0.3647959232330322, 0.3828125, 0.3794037997722626, 0.3601895570755005, 0.3866666555404663, 0.37113404273986816, 0.3762626051902771, 0.38315215706825256, 0.377833753824234, 0.3810741901397705, 0.3967391550540924, 0.38624340295791626], [0.07021792232990265, 0.309927374124527, 0.20823244750499725, 0.33414044976234436, 0.4019370675086975, 0.33171913027763367, 0.37530267238616943, 0.3922518193721771, 0.34624695777893066, 0.35593220591545105, 0.33898305892944336, 0.36803874373435974, 0.35108959674835205, 0.34866830706596375, 0.36077481508255005, 0.34140434861183167, 0.36319613456726074, 0.36077481508255005, 0.35351091623306274, 0.35351091623306274], [0.12159329838296795, 0.3069544497446471, 0.2700157065730409, 0.33454544868127584, 0.3535676239945944, 0.34292866878665723, 0.34949267846491866, 0.35526316261140267, 0.3552794972079009, 0.36888331377796146, 0.35805627304677257, 0.3640718492947308, 0.3680203041277399, 0.35955058610620955, 0.3683559832274242, 0.3610755312460388, 0.3703703751973382, 0.37064677451230665, 0.3738796637981861, 0.3691529896519031])
learning_rates = [0.064, 0.064, 0.064, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.001, 0.001, 0.001, 0.008, 0.008]

# training precision recall is unbalanced (0.75, 0.5, 0.35), test precision recall is balanced (0.35)

plt.figure()
# plot all the training metrics
for i in range(len(train_metrics)):
    if i != 0 and i != 1:# and i != 2 and i != 3:
        plt.plot(train_metrics[i])

# plot all the testing metrics
for i in range(len(test_metrics)):
    if i != 0 and i != 1:# and i != 2 and i != 3:
        plt.plot(test_metrics[i])

# plot all the learning rates
# plt.plot(learning_rates)

plt.legend([# 'train loss', 
            # 'train accuracy', 
            'train precision', 'train recall',
            'train f1',
            # 'test loss', 
            # 'test accuracy', 
            'test precision', 'test recall', 
            'test f1',
            # 'learning rates'
            ])
plt.show()

# abnormal_batches = [[10427, 7685, 10288, 8303, 2707, 6793, 6883, 11040, 6914, 2031, 2462, 8047, 6209, 1774, 10049, 9738, 7025, 7250, 799, 10314, 579, 2967, 547, 2750, 2073, 3427, 6236, 1882, 3849, 4799, 9301, 9935, 3292, 4457, 9553, 7013, 3691, 6685, 8997, 3720, 8448, 3304, 3983, 4678, 8478, 10660, 8399, 3269, 38, 2961, 9059, 9071, 2744, 7665, 2421, 4536, 2912, 5641, 6852, 7230, 7597, 2810, 2451, 3283, 2313, 1859, 581, 3970, 3276, 4508, 6034, 1767, 6016, 1782, 435, 10805, 10647, 10204, 5331, 476, 9772, 10779, 5696, 9331, 9514, 275, 8021, 5182, 3424, 520, 10163, 9123, 10655, 10588, 6314, 5913, 5735, 1627, 8382, 2344, 637, 8306, 8394, 1468, 2345, 7204, 8456, 3088, 4847, 5385, 1122, 6353, 5816, 2977, 332, 893, 5349, 1091, 4289, 10308, 1242, 9986, 1889, 9132, 7415, 10073, 7267], [6683, 941, 1099, 2672, 7100, 10983, 4762, 10964, 9782, 10026, 6583, 7362, 7667, 6600, 8090, 7637, 799, 5835, 4747, 10501, 566, 7485, 9770, 6893, 72, 8671, 10150, 7102, 5232, 10440, 8798, 1306, 9759, 7374, 5124, 7514, 7401, 2598, 10582, 3423, 2748, 5196, 7927, 10839, 2117, 11027, 6425, 805, 7068, 4892, 9462, 2982, 850, 2881, 8240, 9255, 9775, 3712, 7605, 4667, 6476, 4428, 10445, 650, 2756, 9870, 7695, 1859, 2924, 5081, 4341, 9867, 9448, 2072, 4238, 6211, 7826, 3224, 8616, 1716, 14, 9500, 8267, 4604, 7890, 5464, 1380, 2376, 8159, 2482, 8883, 177, 763, 4584, 1769, 8557, 12, 8097, 5591, 3485, 2135, 10951, 3765, 982, 9326, 3983, 10155, 4395, 10924, 8323, 257, 1575, 10464, 1367, 4293, 5145, 10699, 10564, 1811, 6932, 4127, 4219, 1783, 4208, 1432, 8668, 4171, 7676, 9648, 1312, 7887, 11118, 4536, 9533, 1093, 8639, 11060, 580, 979, 8836, 4570, 2850, 4545, 1199, 100, 9993, 2226, 3703, 3426, 6638, 2869, 833, 7777, 6307, 3890, 10508, 5205, 1942, 6399, 5247, 9059, 4630, 9857, 5170, 8478, 7889, 3678, 5570, 2671, 5360, 1690, 4145, 4461, 6582, 2658, 7727, 116, 10911, 8299, 8208, 8350, 7745, 2818, 9566, 3681, 8597, 6062, 9318, 2269, 6091, 1337, 9733, 6894, 3435, 2292, 9000, 7383, 11005, 2310, 5683, 1319, 1071, 397, 2951, 9033, 3658, 6956, 5634, 2421, 8132, 2806, 5980, 4274, 4576, 9637, 706, 3858, 1615, 11116, 4908, 1209, 6342, 6339, 6626, 1595, 10942, 9933, 9602, 4836, 9354, 440, 4376, 9044, 6883, 6694, 10606, 6096, 10936, 6313, 7032, 8346, 6030, 5671, 166, 1895, 8953, 4512, 7150, 7201, 2628, 1409, 6200, 10180, 8768], [10025, 4577, 5460, 8211, 3502, 4838, 2143, 2292, 9424, 8493, 7474, 8686, 9516, 3864, 5593, 833, 675, 5911, 3925, 1165, 1120, 7369, 5723, 7849, 4440, 6963, 7627, 1156, 1523, 2016, 9289, 2097, 5453, 4435, 9136, 1409, 9463, 2881, 10219, 4542, 8119, 2391, 3765, 5040, 6206, 8262, 6437, 8051, 9118, 8798, 1859, 9662, 3807, 10492, 5909, 9438, 8353, 2246, 9488, 3723, 9472, 865, 8212, 8886, 9301, 7704, 5544, 570, 9588, 7955, 6531, 2897, 5298, 2811, 9334, 1987, 6370, 10082, 1821, 13, 307, 6640, 507, 4409, 10525, 2864, 8120, 805, 3681, 10810, 9647, 1407, 3191, 3393, 841, 1774, 7540, 720, 5145, 4003, 10075, 3100, 4462, 7170, 9745, 3513, 10453, 4737, 8360, 1897, 4001, 6228, 2899, 7481, 7076, 1174, 1309, 5548, 1913, 4367, 7464, 8134, 9821, 11060, 7038, 7194, 7904, 3575, 8439, 10920, 5119, 8278, 8555, 6093, 4563, 5528, 939, 10150, 2852, 7936, 10259, 4589, 5431, 1439, 10172, 4156, 6805, 9205, 8305, 10283, 1500, 9383, 3768, 1376, 8768, 5265, 1124, 9614, 6630, 2211, 10305, 6185, 2932, 8586, 3536, 5888, 7215, 7055, 4171, 3551, 6371, 1584, 4904, 9918, 3818, 37, 1399, 10885, 6757, 2250, 4558, 10988, 7150, 3094, 3435, 580, 10482, 4857, 2521, 9916, 673, 9864, 2754, 9163, 6183, 7201, 3143, 9494, 806, 10921, 10301, 7670, 2498, 1019, 4518, 10397, 11070, 852, 7765, 4529, 3745, 3951, 3760, 7726, 763, 5299, 3334, 2943, 7713, 1074, 5771, 3876, 9036, 6641, 9147, 5994, 1964, 6075, 7214, 6884, 6158, 1180, 3522, 8455, 7746, 6129, 7500, 3582, 9623, 9968, 3966, 9972, 4600, 3980, 6124, 3105, 4504, 4654, 7884, 4514, 2040, 5821, 959, 1655, 1835, 414, 5132, 8184, 1630, 1371, 4366, 5139, 1504, 5030, 4936, 9693, 2147, 3532, 527, 6797, 4259, 9618, 4736, 5549, 2076, 10652, 10544, 5570, 8281, 4449, 6781, 8159, 2622, 519, 6996, 6575, 1475, 9935, 5478, 463, 10262, 5979, 3419, 2956, 11102, 6399, 10421, 7666, 7889, 2785, 7695, 735, 449, 6718, 9613, 5832, 4502, 2421, 1075, 1282, 8017, 873, 4334, 6684, 10934, 2126, 4535, 10008], [3944, 4802, 10971, 7986, 3411, 5991, 4920, 9093, 3796, 1596, 3234, 6260, 5741, 5568, 5716, 141, 4585, 5430, 7069, 3, 520, 1211, 931, 137, 6784, 1352, 4049, 5736, 7451, 1975, 4678, 7429, 9012, 10377, 1483, 7622, 9962, 5012, 2678, 2578, 7216, 10328, 6456, 600, 4867, 6638, 4, 7640, 8749, 5796, 3791, 6610, 8208, 7835, 8295, 2781, 2947, 451, 284, 8188, 6168, 9131, 1008, 10373, 9637, 9402, 4466, 9535, 11046, 476, 7853, 9665, 2403, 1233, 3318, 3263, 527, 4238, 8439, 799, 7679, 5497, 3134, 1841, 4519, 3286, 8202, 7161, 9857, 5002, 4251, 10531, 9054, 9506, 10806, 8713, 10704, 2249, 4499, 9018, 4716, 3292, 4005, 7012, 9588, 7908, 1576, 10647, 5881, 1142, 803, 10670, 5, 1605, 5414, 1012, 2126, 4508, 2271, 9373, 275, 1904, 7629, 1260, 2840, 9020, 9334, 1660, 945, 977, 2421, 6519, 3623, 3875, 2091, 2315, 5249, 10814, 4907, 786, 9301, 4202, 4168, 6379, 7275, 4146, 6399, 1621, 9924, 1324, 4979, 9938, 9789, 1093, 4370, 7150, 4880, 1471, 652, 2851, 4378, 8087, 3300, 4299, 4235, 4434, 6598, 6721, 1050, 1622, 4110, 3369, 1814, 9364, 1434, 1432, 3782, 3426, 3268, 5372, 9860, 9785, 8699, 7006, 4200, 5370, 2744, 8072, 4589, 6793, 6538, 4559, 3980, 2839, 6187, 3677, 10092, 1320, 6117, 6179, 1859, 4689, 689, 2576, 176, 8969, 1046, 8975, 2624, 5259, 4916, 6728, 6945, 5793, 6047, 5811, 3011, 4212, 9618, 1513, 9147, 4001, 3506, 4531, 8643, 10093, 10237, 5964, 2076, 7785, 5944, 8522, 6803, 10659, 1419, 8163, 5289, 4006, 4226, 9116, 7282, 7907, 3157, 534, 1167, 593, 4969, 6173, 5895, 4597, 10636, 9669, 7596, 1196, 5513, 5381, 2150, 7373, 9023, 4373, 3294, 4588, 4703, 5825, 5396, 1174, 3128, 1317, 5629, 871, 5697, 8581, 8702, 283, 9768, 1625, 383, 9042, 3069, 2778, 768, 1231, 3392, 6530, 5771, 9461, 5214, 4726, 1890, 540, 6908, 4650, 10922, 10658, 1716, 2852, 742, 7140, 8030, 10368, 8656, 6837, 9335, 4331, 4375, 5498, 1179, 1246, 100, 733, 6990, 2707, 6404, 11064, 9489, 6327, 8534, 1581, 6943, 6627, 1629, 817, 6346, 4099, 4071, 3007, 3899, 4198, 2316, 7120, 4313, 10882, 637, 351, 9563, 5141, 3850, 2561, 7537, 7046, 2, 1107, 828, 10174, 8297, 11060, 10103, 6411, 253, 475, 4737, 6000, 9792, 2598, 3301, 2169, 3004, 1897, 4062, 10439, 1736, 790, 826, 1444, 7546, 5911, 4756, 1099, 10738, 7887, 10200, 5994, 6649, 5889, 2461, 840, 7554, 2057, 4044, 1665, 2527, 1282], [10090, 5232, 11060, 11109, 4023, 7251, 4184, 1575, 2421, 3908, 9130, 4900, 7962, 5596, 2634, 941, 5849, 8997, 8200, 10163, 2076, 4187, 5957, 268, 6834, 5539, 2693, 1777, 4558, 70, 3315, 2711, 10241, 8428, 2482, 4422, 8292, 9720, 5591, 5585, 7416, 7588, 7556, 1024, 4259, 1102, 8256, 10308, 595, 7658, 1219, 1546, 9782, 4221, 3676, 8568, 3426, 699, 10597, 11079, 5848, 4015, 8530, 9479, 1209, 6292, 3572, 9863, 7569, 141, 9649, 871, 9662, 10329, 2669, 5081, 2823, 50, 8980, 6437, 8632, 1085, 6050, 7737, 6433, 6608, 3132, 10957, 4870, 4720, 841, 2992, 10507, 6103, 5460, 7746, 2485, 5124, 2675, 6776, 154, 7429, 9044, 2828, 5236, 8410, 7324, 698, 9807, 4071, 489, 10715, 10485, 9962, 9619, 335, 11000, 1155, 468, 2097, 2850, 446, 8257, 10410, 7263, 10948, 7441, 4298, 4933, 5143, 4340, 10162, 3965, 2271, 8770, 5349, 5470, 7452, 9469, 7239, 8886, 5816, 5739, 440, 7947, 8457, 10234, 7361, 7877, 4429, 6094, 8373, 4724, 7189, 9667, 7372, 6837, 8488, 5505, 5073, 8581, 4964, 8512, 10259, 3015, 438, 7249, 9759, 5426, 8585, 8618, 4690, 2046, 3629, 3271, 10110, 4504, 4630, 937, 1859, 4338, 2318, 2148, 3660, 572, 9986, 1342, 7592, 4204, 1965, 7362]]
# counter = Counter()
# for cur_list in abnormal_batches:
#     counter.update(set(cur_list))
# fishy_examples = [elem for elem, count in counter.items() if count >= 3]
# print(fishy_examples)

# all_data = MyDataset([',', '\t'], ['data/kaggle spam.csv', 'data/UC Irvine collection/SMSSpamCollection'])
# for i in fishy_examples:
#     print(f'{all_data[i][2]}, {all_data[i][3]}')